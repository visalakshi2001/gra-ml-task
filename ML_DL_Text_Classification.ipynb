{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 323,
      "id": "241b84fa",
      "metadata": {
        "id": "241b84fa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import Precision, Recall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_split_data(return_one_hot = True, random_state=101):\n",
        "    df = pd.read_csv('/content/Problem_Dataset.csv')\n",
        "\n",
        "    if return_one_hot:\n",
        "        df_ = pd.DataFrame()\n",
        "        df_[\"Obs\"] = df[\"Obs\"]\n",
        "        df_[\"Type\"] = [arr for arr in df[df[\"Type\"].unique()].values]\n",
        "    else:\n",
        "        df_ = df[[\"Obs\", \"Type\"]]\n",
        "\n",
        "    train_data, val_data = train_test_split(df_, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    return train_data, val_data\n",
        "\n",
        "def tokenize_and_fit(docs):\n",
        "    # Tokenize your text data\n",
        "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(docs)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    max_len = max(len(sentence.split()) for sentence in docs)\n",
        "\n",
        "    return tokenizer, max_len\n",
        "\n",
        "\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    # pad sequences\n",
        "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "    return padded\n",
        "\n",
        "def create_multi_label_model(vocab_size, max_len, class_to_index: dict, embedding_dim=50, ensemble=False):\n",
        "\n",
        "    if ensemble:\n",
        "        model = keras.Sequential([\n",
        "            layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    else:\n",
        "        num_classes = len(class_to_index.keys())\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(num_classes, activation='sigmoid')  # Use sigmoid activation for multi-label classification\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MlpK7OwH1yuj"
      },
      "id": "MlpK7OwH1yuj",
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1229a4f9",
      "metadata": {
        "id": "1229a4f9"
      },
      "source": [
        "## First Pass (Ensemble)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Problem_Dataset.csv')"
      ],
      "metadata": {
        "id": "z7LuYQI4-MMg"
      },
      "id": "z7LuYQI4-MMg",
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_index = dict(zip(df['Type'].unique(), range(len(df['Type'].unique()))))"
      ],
      "metadata": {
        "id": "1tuL65DUBhxF"
      },
      "id": "1tuL65DUBhxF",
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_train, ensemble_val = read_and_split_data(return_one_hot=False)"
      ],
      "metadata": {
        "id": "D1UXtQCD9GHJ"
      },
      "id": "D1UXtQCD9GHJ",
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "id": "418f1d7a",
      "metadata": {
        "id": "418f1d7a"
      },
      "outputs": [],
      "source": [
        "# Tokenize your text data\n",
        "tokenizer, max_len = tokenize_and_fit(ensemble_train['Obs'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "8f3cfea0",
      "metadata": {
        "id": "8f3cfea0",
        "outputId": "a0cb0e0d-ef4c-4dc0-f95b-b24d949142dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "21/21 [==============================] - 6s 160ms/step - loss: 0.4594 - accuracy: 0.8304 - val_loss: 0.3669 - val_accuracy: 0.8690\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 2s 83ms/step - loss: 0.3304 - accuracy: 0.8586 - val_loss: 0.3036 - val_accuracy: 0.8690\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 29ms/step - loss: 0.2268 - accuracy: 0.8943 - val_loss: 0.2181 - val_accuracy: 0.8929\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 46ms/step - loss: 0.1174 - accuracy: 0.9673 - val_loss: 0.1474 - val_accuracy: 0.9345\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.0407 - accuracy: 0.9985 - val_loss: 0.1067 - val_accuracy: 0.9643\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 3s 97ms/step - loss: 0.4608 - accuracy: 0.8393 - val_loss: 0.5291 - val_accuracy: 0.8214\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.3604 - accuracy: 0.8735 - val_loss: 0.4352 - val_accuracy: 0.8214\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.2617 - accuracy: 0.8750 - val_loss: 0.3278 - val_accuracy: 0.8214\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.1623 - accuracy: 0.9226 - val_loss: 0.2435 - val_accuracy: 0.8452\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 0s 18ms/step - loss: 0.1011 - accuracy: 0.9821 - val_loss: 0.1894 - val_accuracy: 0.9048\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 3s 79ms/step - loss: 0.4959 - accuracy: 0.8348 - val_loss: 0.3858 - val_accuracy: 0.8750\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 1s 67ms/step - loss: 0.3751 - accuracy: 0.8542 - val_loss: 0.3387 - val_accuracy: 0.8750\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 36ms/step - loss: 0.2753 - accuracy: 0.8601 - val_loss: 0.2637 - val_accuracy: 0.8810\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 31ms/step - loss: 0.1862 - accuracy: 0.9182 - val_loss: 0.2119 - val_accuracy: 0.8869\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 0.1120 - accuracy: 0.9673 - val_loss: 0.1781 - val_accuracy: 0.9226\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 3s 106ms/step - loss: 0.4829 - accuracy: 0.8304 - val_loss: 0.4709 - val_accuracy: 0.8333\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 1s 43ms/step - loss: 0.3465 - accuracy: 0.8631 - val_loss: 0.3518 - val_accuracy: 0.8333\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 48ms/step - loss: 0.2182 - accuracy: 0.8661 - val_loss: 0.2360 - val_accuracy: 0.8452\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 28ms/step - loss: 0.1244 - accuracy: 0.9628 - val_loss: 0.1822 - val_accuracy: 0.9226\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 1s 36ms/step - loss: 0.0643 - accuracy: 0.9926 - val_loss: 0.1454 - val_accuracy: 0.9524\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 2s 66ms/step - loss: 0.4585 - accuracy: 0.8214 - val_loss: 0.3503 - val_accuracy: 0.8571\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.3156 - accuracy: 0.8571 - val_loss: 0.2806 - val_accuracy: 0.8631\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 44ms/step - loss: 0.2052 - accuracy: 0.8973 - val_loss: 0.2032 - val_accuracy: 0.9048\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 0.1003 - accuracy: 0.9777 - val_loss: 0.1221 - val_accuracy: 0.9464\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 1s 26ms/step - loss: 0.0282 - accuracy: 0.9970 - val_loss: 0.0835 - val_accuracy: 0.9643\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 3s 121ms/step - loss: 0.4869 - accuracy: 0.8467 - val_loss: 0.4265 - val_accuracy: 0.8571\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 2s 88ms/step - loss: 0.3772 - accuracy: 0.8542 - val_loss: 0.3800 - val_accuracy: 0.8571\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 54ms/step - loss: 0.2858 - accuracy: 0.8557 - val_loss: 0.3294 - val_accuracy: 0.8571\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 29ms/step - loss: 0.1831 - accuracy: 0.9048 - val_loss: 0.3173 - val_accuracy: 0.8810\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.1187 - accuracy: 0.9658 - val_loss: 0.3157 - val_accuracy: 0.8988\n",
            "Epoch 1/5\n",
            "21/21 [==============================] - 3s 119ms/step - loss: 0.4949 - accuracy: 0.8110 - val_loss: 0.3511 - val_accuracy: 0.8869\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 1s 70ms/step - loss: 0.3950 - accuracy: 0.8452 - val_loss: 0.3244 - val_accuracy: 0.8869\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 1s 33ms/step - loss: 0.2934 - accuracy: 0.8467 - val_loss: 0.2572 - val_accuracy: 0.8869\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 1s 30ms/step - loss: 0.1869 - accuracy: 0.8854 - val_loss: 0.1979 - val_accuracy: 0.9107\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 1s 34ms/step - loss: 0.1070 - accuracy: 0.9807 - val_loss: 0.1693 - val_accuracy: 0.9345\n"
          ]
        }
      ],
      "source": [
        "# Train a model for each label\n",
        "models = {}\n",
        "for label in df['Type'].unique():\n",
        "\n",
        "    X_data = encode_docs(tokenizer, max_len, ensemble_train[\"Obs\"])\n",
        "    binary_labels = (ensemble_train['Type'] == label).astype(int)\n",
        "    y_data = np.asarray(binary_labels)\n",
        "\n",
        "    model = create_multi_label_model(vocab_size=vocab_size,\n",
        "                                     ensemble=True,\n",
        "                                     class_to_index=class_to_index,\n",
        "                                     max_len=max_len)\n",
        "    model.fit(X_data, y_data, epochs=5, batch_size=32, validation_split=0.2)\n",
        "    models[label] = model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "id": "26a2a26b",
      "metadata": {
        "id": "26a2a26b"
      },
      "outputs": [],
      "source": [
        "def save_model_s(models, name_prefix):\n",
        "    if type(models) == dict:\n",
        "        for l,m in zip(models.keys(),models.values()):\n",
        "            m.save(f'{name_prefix}_{l}.h5')\n",
        "    else:\n",
        "        models.save(f\"{name_prefix}.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_s(name_prefix, class_to_index, ensemble=True):\n",
        "    from tensorflow.keras.models import load_model\n",
        "\n",
        "    if ensemble:\n",
        "        models={}\n",
        "        for l in class_to_index.keys():\n",
        "            m = load_model(f\"{name_prefix}_{l}.h5\")\n",
        "            models[l] = m\n",
        "        return models\n",
        "    else:\n",
        "        model = load_model(f\"{name_prefix}.h5\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "xzcBHFH0EasX"
      },
      "id": "xzcBHFH0EasX",
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_s(models, \"ensemble_model\")"
      ],
      "metadata": {
        "id": "xMXTXfGRKQUg"
      },
      "id": "xMXTXfGRKQUg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "id": "fae13415",
      "metadata": {
        "id": "fae13415"
      },
      "outputs": [],
      "source": [
        "# Load all the models into an ensemble\n",
        "\n",
        "models = load_model_s(ensemble=True, name_prefix=\"ensemble_model\", class_to_index=class_to_index)\n",
        "\n",
        "def get_predictions(model, val_data, ensemble=True):\n",
        "\n",
        "    if ensemble:\n",
        "        ensemble_predictions = []\n",
        "        for model_label, model in models.items():\n",
        "            val_predictions = model.predict(encode_docs(tokenizer, max_len, val_data[\"Obs\"]))\n",
        "            ensemble_predictions.append(val_predictions)\n",
        "\n",
        "        # Create an array of predictions for each input text\n",
        "        ensemble_predictions = np.array(ensemble_predictions).squeeze().T\n",
        "        # Threshold ensemble predictions for binary classification\n",
        "        binary_ensemble_predictions = (ensemble_predictions > 0.5).astype(int)\n",
        "\n",
        "        y_hat = np.argmax((ensemble_predictions > 0.5).astype(int), axis=1)\n",
        "\n",
        "        return y_hat\n",
        "    else:\n",
        "\n",
        "        multiclass_predictions = multi_label_model.predict(encode_docs(tokenizer, max_len, val_data['Obs']))\n",
        "        y_hat = (multiclass_predictions > 0.5).astype(int)\n",
        "\n",
        "\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "id": "cee74b02",
      "metadata": {
        "id": "cee74b02"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(val_data, y_hat, ensemble=True):\n",
        "\n",
        "    if ensemble:\n",
        "        y_val = val_data[\"Type\"].apply(lambda x: list(models.keys()).index(x)).values\n",
        "    else:\n",
        "        # Predictions on validation set\n",
        "        y_val = np.array([np.asarray(x, dtype = np.float64) for x in val_data[\"Type\"]])\n",
        "\n",
        "\n",
        "    # Obtain precision, recall, F1-score, and print the classification report\n",
        "    print(\"Ensemble Classification Report:\")\n",
        "    print(classification_report(y_val, y_hat, target_names=models.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "id": "6992fa77",
      "metadata": {
        "id": "6992fa77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd8d454-c804-4af9-923c-a717a5edd168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_hat = get_predictions(models, ensemble_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "id": "792344a5",
      "metadata": {
        "id": "792344a5",
        "outputId": "d34aea18-7653-4eea-f1ea-9f1f24a83550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          B3       0.28      0.94      0.44        33\n",
            "          B4       0.88      0.20      0.33        35\n",
            "          A3       0.69      0.35      0.47        31\n",
            "          B1       0.96      0.77      0.85        30\n",
            "          B2       0.96      0.81      0.88        27\n",
            "          A1       0.88      0.25      0.39        28\n",
            "          A2       0.64      0.54      0.58        26\n",
            "\n",
            "    accuracy                           0.55       210\n",
            "   macro avg       0.75      0.55      0.56       210\n",
            "weighted avg       0.75      0.55      0.55       210\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(ensemble_val, y_hat, ensemble=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b09cdc",
      "metadata": {
        "id": "a1b09cdc"
      },
      "source": [
        "## Second Pass (Multi-label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "0f96c6d9",
      "metadata": {
        "id": "0f96c6d9"
      },
      "outputs": [],
      "source": [
        "multiclass_train, multiclass_val = read_and_split_data(return_one_hot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09ed863",
      "metadata": {
        "id": "a09ed863"
      },
      "outputs": [],
      "source": [
        "# Tokenize your text data\n",
        "tokenizer, max_len = tokenize_and_fit(multiclass_train[\"Obs\"])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "id": "b2e0b5ee",
      "metadata": {
        "scrolled": true,
        "id": "b2e0b5ee",
        "outputId": "8fe5e2ef-c68d-4b74-ceea-736f9f54b9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - 4s 154ms/step - loss: 0.5291 - accuracy: 0.1548 - val_loss: 0.4174 - val_accuracy: 0.1250\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - 1s 53ms/step - loss: 0.4073 - accuracy: 0.3199 - val_loss: 0.3998 - val_accuracy: 0.4226\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - 1s 37ms/step - loss: 0.3732 - accuracy: 0.7202 - val_loss: 0.3787 - val_accuracy: 0.4345\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - 1s 24ms/step - loss: 0.3277 - accuracy: 0.7634 - val_loss: 0.3419 - val_accuracy: 0.5893\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - 1s 39ms/step - loss: 0.2611 - accuracy: 0.8705 - val_loss: 0.3003 - val_accuracy: 0.5774\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - 0s 19ms/step - loss: 0.1887 - accuracy: 0.9256 - val_loss: 0.2595 - val_accuracy: 0.6429\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - 0s 17ms/step - loss: 0.1272 - accuracy: 0.9613 - val_loss: 0.2260 - val_accuracy: 0.7262\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - 0s 17ms/step - loss: 0.0842 - accuracy: 0.9762 - val_loss: 0.2122 - val_accuracy: 0.7560\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 0.0553 - accuracy: 0.9911 - val_loss: 0.2058 - val_accuracy: 0.7500\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 0.0375 - accuracy: 0.9911 - val_loss: 0.2019 - val_accuracy: 0.7857\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 0.9970 - val_loss: 0.2025 - val_accuracy: 0.7917\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.9985 - val_loss: 0.2040 - val_accuracy: 0.7976\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - 0s 24ms/step - loss: 0.0139 - accuracy: 0.9985 - val_loss: 0.2112 - val_accuracy: 0.7917\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2137 - val_accuracy: 0.8036\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2176 - val_accuracy: 0.8036\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c9891d4e1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ],
      "source": [
        "# Train a multi-label model\n",
        "X_data = encode_docs(tokenizer, max_len, multiclass_train[\"Obs\"])\n",
        "binary_labels = [np.asarray(x, dtype = np.float64) for x in multiclass_train[\"Type\"]]\n",
        "y_data = np.array(binary_labels)\n",
        "\n",
        "multi_label_model = create_multi_label_model(vocab_size=vocab_size,\n",
        "                                             max_len=max_len,\n",
        "                                             class_to_index=class_to_index,\n",
        "                                             ensemble=False)\n",
        "multi_label_model.fit(X_data, y_data, epochs=15, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_s(multi_label_model, \"multilabel_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uonp72M4Kbns",
        "outputId": "bce8a439-a612-408d-f4f7-fed5f68fc9b3"
      },
      "id": "Uonp72M4Kbns",
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "id": "291b7a66",
      "metadata": {
        "id": "291b7a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a06047-38d4-4eae-8c70-5e882a5fbd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = load_model_s(ensemble=False, name_prefix='multilabel_model', class_to_index=class_to_index)\n",
        "\n",
        "y_hat = get_predictions(model, multiclass_val, ensemble=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "id": "b2167567",
      "metadata": {
        "id": "b2167567",
        "outputId": "c29896b5-df47-489f-ca55-fcc50ef384c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          B3       0.94      0.48      0.64        33\n",
            "          B4       0.84      0.46      0.59        35\n",
            "          A3       0.73      0.26      0.38        31\n",
            "          B1       0.95      0.63      0.76        30\n",
            "          B2       0.94      0.59      0.73        27\n",
            "          A1       0.67      0.36      0.47        28\n",
            "          A2       0.68      0.50      0.58        26\n",
            "\n",
            "   micro avg       0.83      0.47      0.60       210\n",
            "   macro avg       0.82      0.47      0.59       210\n",
            "weighted avg       0.83      0.47      0.59       210\n",
            " samples avg       0.47      0.47      0.47       210\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(y_hat=y_hat, val_data=multiclass_val, ensemble=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ad42ce",
      "metadata": {
        "id": "99ad42ce"
      },
      "source": [
        "## Third Pass (BERT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "epQdzfwcH1R3"
      },
      "id": "epQdzfwcH1R3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_train, bert_val = read_and_split_data(return_one_hot=True)"
      ],
      "metadata": {
        "id": "KwvZWARHIZ5r"
      },
      "id": "KwvZWARHIZ5r",
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # You can try other BERT models as well\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=len(df['Type'].unique()))\n",
        "\n",
        "\n",
        "train_inputs = tokenizer.batch_encode_plus(list(train_data['Obs']), add_special_tokens=True, padding=True, truncation=True, return_tensors='np', max_length=tokenizer.model_max_length)\n",
        "val_inputs = tokenizer.batch_encode_plus(list(val_data['Obs']), add_special_tokens=True, padding=True, truncation=True, return_tensors='np', max_length=tokenizer.model_max_length)\n",
        "\n",
        "\n",
        "train_labels = tf.convert_to_tensor([label for label in train_data[\"Type\"]])\n",
        "val_labels = tf.convert_to_tensor([label for label in val_data[\"Type\"]])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8tNrwAzmdiiG"
      },
      "id": "8tNrwAzmdiiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[Precision(), Recall()])\n",
        "\n",
        "history = model.fit(dict(train_inputs), train_labels,\n",
        "                    epochs=10,\n",
        "                    batch_size=8,\n",
        "                    validation_data=(dict(val_inputs), val_labels)\n",
        "          )"
      ],
      "metadata": {
        "id": "SC1RONlSf7Dl"
      },
      "id": "SC1RONlSf7Dl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('/content/fine_tuned_bert_model_tf')\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model_tf = TFBertForSequenceClassification.from_pretrained('/content/fine_tuned_bert_model_tf')\n",
        "\n"
      ],
      "metadata": {
        "id": "OJLPRfsjdwhi"
      },
      "id": "OJLPRfsjdwhi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the validation set\n",
        "val_preds_tf = fine_tuned_model_tf.predict(dict(val_inputs))['logits']\n",
        "val_preds_tf = tf.math.sigmoid(val_preds_tf)\n",
        "\n",
        "# Threshold predictions for binary classification\n",
        "binary_val_preds_tf = (val_preds_tf > 0.5).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUzcnUnZfAOR",
        "outputId": "84a83b70-a8fc-45ed-db8c-0fc60dbd547c"
      },
      "id": "mUzcnUnZfAOR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 8s 99ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Obtain precision, recall, F1-score, and print the classification report\n",
        "print(\"Fine-tuned BERT Classification Report:\")\n",
        "print(classification_report(val_labels.numpy(), binary_val_preds_tf, target_names=df['Type'].explode().unique()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJCHT6RofA38",
        "outputId": "0128cc9d-9a82-4a7f-e825-32342ccd451a"
      },
      "id": "pJCHT6RofA38",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned BERT Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          B3       0.70      0.97      0.81        36\n",
            "          B4       0.61      1.00      0.76        27\n",
            "          A3       0.65      0.90      0.76        31\n",
            "          B1       0.79      0.94      0.86        35\n",
            "          B2       0.48      1.00      0.65        26\n",
            "          A1       0.44      0.93      0.60        29\n",
            "          A2       0.67      0.92      0.77        26\n",
            "\n",
            "   micro avg       0.61      0.95      0.74       210\n",
            "   macro avg       0.62      0.95      0.74       210\n",
            "weighted avg       0.63      0.95      0.75       210\n",
            " samples avg       0.72      0.95      0.79       210\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9f9PotEQroCw"
      },
      "id": "9f9PotEQroCw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}