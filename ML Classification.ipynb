{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a9c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\softwares\\python3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "d:\\softwares\\python3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55db042",
   "metadata": {},
   "source": [
    "## First Pass (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40dd64a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>Obs</th>\n",
       "      <th>Type</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150668</td>\n",
       "      <td>Observed child fixated on a particular texture...</td>\n",
       "      <td>B3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150409</td>\n",
       "      <td>Patient's focus centers on vacuum cleaners, st...</td>\n",
       "      <td>B3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150264</td>\n",
       "      <td>Displays a strong interest in smelling various...</td>\n",
       "      <td>B4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150373</td>\n",
       "      <td>Patient's attachment to a specific book is evi...</td>\n",
       "      <td>B3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150343</td>\n",
       "      <td>Limited awareness of personal boundaries, inva...</td>\n",
       "      <td>A3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>150491</td>\n",
       "      <td>Displays a need for consistency in clothing ch...</td>\n",
       "      <td>B2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>150884</td>\n",
       "      <td>Patient shows a strong attachment to a single ...</td>\n",
       "      <td>B3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>151017</td>\n",
       "      <td>Child displays a tendency to focus on irreleva...</td>\n",
       "      <td>A2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>150297</td>\n",
       "      <td>Displays a tendency to monologue about persona...</td>\n",
       "      <td>A3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>150420</td>\n",
       "      <td>Engages in persistent rocking back and forth w...</td>\n",
       "      <td>B1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sID                                                Obs Type  A1  A2  \\\n",
       "0     150668  Observed child fixated on a particular texture...   B3   0   0   \n",
       "1     150409  Patient's focus centers on vacuum cleaners, st...   B3   0   0   \n",
       "2     150264  Displays a strong interest in smelling various...   B4   0   0   \n",
       "3     150373  Patient's attachment to a specific book is evi...   B3   0   0   \n",
       "4     150343  Limited awareness of personal boundaries, inva...   A3   0   0   \n",
       "...      ...                                                ...  ...  ..  ..   \n",
       "1045  150491  Displays a need for consistency in clothing ch...   B2   0   0   \n",
       "1046  150884  Patient shows a strong attachment to a single ...   B3   0   0   \n",
       "1047  151017  Child displays a tendency to focus on irreleva...   A2   0   1   \n",
       "1048  150297  Displays a tendency to monologue about persona...   A3   0   0   \n",
       "1049  150420  Engages in persistent rocking back and forth w...   B1   0   0   \n",
       "\n",
       "      A3  B1  B2  B3  B4  \n",
       "0      0   0   0   1   0  \n",
       "1      0   0   0   1   0  \n",
       "2      0   0   0   0   1  \n",
       "3      0   0   0   1   0  \n",
       "4      1   0   0   0   0  \n",
       "...   ..  ..  ..  ..  ..  \n",
       "1045   0   0   1   0   0  \n",
       "1046   0   0   0   1   0  \n",
       "1047   0   0   0   0   0  \n",
       "1048   1   0   0   0   0  \n",
       "1049   0   1   0   0   0  \n",
       "\n",
       "[1050 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "# Assuming you have a DataFrame with 'text' column and 'label' column\n",
    "# where 'label' contains A1, A2, A3, B1, B2, B3, B4 labels\n",
    "# Replace 'your_dataset.csv' with your actual dataset file\n",
    "df = pd.read_csv('Problem_Dataset.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8915ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[[\"Obs\", \"Type\"]]\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(df_, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d8e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your text data\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['Obs'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(len(sentence.split()) for sentence in df['Obs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bab5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple deep learning model\n",
    "def create_model(embedding_dim=50):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e7d9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84f94784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12, 427,  87, ...,   0,   0,   0],\n",
       "       [ 14,  17,   5, ...,   0,   0,   0],\n",
       "       [ 12,   5,  32, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 14,  45,   2, ...,   0,   0,   0],\n",
       "       [523, 866,   1, ...,   0,   0,   0],\n",
       "       [ 11,   5,  48, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_docs(tokenizer, max_len, train_data[\"Obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "980a8725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B3', 'B4', 'A3', 'B1', 'B2', 'A1', 'A2'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['Type'].unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e31dc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 9ms/sample - loss: 0.4630 - accuracy: 0.8289 - val_loss: 0.3605 - val_accuracy: 0.8690\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 1s 813us/sample - loss: 0.3332 - accuracy: 0.8601 - val_loss: 0.2964 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 315us/sample - loss: 0.2153 - accuracy: 0.8988 - val_loss: 0.2107 - val_accuracy: 0.9167\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 272us/sample - loss: 0.1059 - accuracy: 0.9836 - val_loss: 0.1414 - val_accuracy: 0.9405\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 276us/sample - loss: 0.0324 - accuracy: 0.9985 - val_loss: 0.1069 - val_accuracy: 0.9702\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 1ms/sample - loss: 0.4687 - accuracy: 0.8423 - val_loss: 0.5391 - val_accuracy: 0.8214\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 258us/sample - loss: 0.3642 - accuracy: 0.8735 - val_loss: 0.4427 - val_accuracy: 0.8214\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 333us/sample - loss: 0.2910 - accuracy: 0.8735 - val_loss: 0.3658 - val_accuracy: 0.8214\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 313us/sample - loss: 0.1840 - accuracy: 0.9077 - val_loss: 0.2530 - val_accuracy: 0.8452\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 336us/sample - loss: 0.0866 - accuracy: 0.9821 - val_loss: 0.1449 - val_accuracy: 0.9345\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 2ms/sample - loss: 0.4684 - accuracy: 0.8512 - val_loss: 0.3889 - val_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 283us/sample - loss: 0.3741 - accuracy: 0.8542 - val_loss: 0.3311 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 278us/sample - loss: 0.2683 - accuracy: 0.8542 - val_loss: 0.2643 - val_accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 277us/sample - loss: 0.1865 - accuracy: 0.8676 - val_loss: 0.2218 - val_accuracy: 0.8810\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 298us/sample - loss: 0.1382 - accuracy: 0.9539 - val_loss: 0.1967 - val_accuracy: 0.9107\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 2ms/sample - loss: 0.4993 - accuracy: 0.8289 - val_loss: 0.4761 - val_accuracy: 0.8333\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 288us/sample - loss: 0.3628 - accuracy: 0.8631 - val_loss: 0.3859 - val_accuracy: 0.8333\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 280us/sample - loss: 0.2498 - accuracy: 0.8631 - val_loss: 0.2815 - val_accuracy: 0.8333\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 265us/sample - loss: 0.1447 - accuracy: 0.8646 - val_loss: 0.2215 - val_accuracy: 0.8333\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 303us/sample - loss: 0.1094 - accuracy: 0.9554 - val_loss: 0.2131 - val_accuracy: 0.8869\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 2ms/sample - loss: 0.4632 - accuracy: 0.8185 - val_loss: 0.3599 - val_accuracy: 0.8571\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 272us/sample - loss: 0.3293 - accuracy: 0.8571 - val_loss: 0.2829 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 273us/sample - loss: 0.2085 - accuracy: 0.9152 - val_loss: 0.1864 - val_accuracy: 0.9167\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 291us/sample - loss: 0.0835 - accuracy: 0.9851 - val_loss: 0.0966 - val_accuracy: 0.9643\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 352us/sample - loss: 0.0233 - accuracy: 0.9970 - val_loss: 0.0762 - val_accuracy: 0.9643\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 2ms/sample - loss: 0.4913 - accuracy: 0.8244 - val_loss: 0.4275 - val_accuracy: 0.8571\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 277us/sample - loss: 0.3799 - accuracy: 0.8542 - val_loss: 0.3809 - val_accuracy: 0.8571\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 292us/sample - loss: 0.2901 - accuracy: 0.8557 - val_loss: 0.3327 - val_accuracy: 0.8571\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 347us/sample - loss: 0.1929 - accuracy: 0.9226 - val_loss: 0.3323 - val_accuracy: 0.8750\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 300us/sample - loss: 0.1223 - accuracy: 0.9702 - val_loss: 0.3254 - val_accuracy: 0.8929\n",
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 1s 1ms/sample - loss: 0.4905 - accuracy: 0.8199 - val_loss: 0.3479 - val_accuracy: 0.8869\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 0s 255us/sample - loss: 0.3791 - accuracy: 0.8438 - val_loss: 0.3298 - val_accuracy: 0.8869\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 0s 271us/sample - loss: 0.2796 - accuracy: 0.8467 - val_loss: 0.2440 - val_accuracy: 0.8869\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 0s 294us/sample - loss: 0.1688 - accuracy: 0.9301 - val_loss: 0.1961 - val_accuracy: 0.9286\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 0s 275us/sample - loss: 0.0987 - accuracy: 0.9821 - val_loss: 0.1735 - val_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "# Train a model for each label\n",
    "models = {}\n",
    "for label in df_['Type'].unique():\n",
    "    binary_labels = (train_data['Type'] == label).astype(int)\n",
    "    X_data = encode_docs(tokenizer, max_len, train_data[\"Obs\"])\n",
    "    y_data = np.asarray(binary_labels)\n",
    "    model = create_model()\n",
    "    model.fit(X_data, y_data, epochs=5, batch_size=32, validation_split=0.2)\n",
    "    models[label] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3325829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B3': <tensorflow.python.keras.engine.sequential.Sequential at 0x20790227088>,\n",
       " 'B4': <tensorflow.python.keras.engine.sequential.Sequential at 0x20791ff7e08>,\n",
       " 'A3': <tensorflow.python.keras.engine.sequential.Sequential at 0x20791470508>,\n",
       " 'B1': <tensorflow.python.keras.engine.sequential.Sequential at 0x20791259f48>,\n",
       " 'B2': <tensorflow.python.keras.engine.sequential.Sequential at 0x207914e85c8>,\n",
       " 'A1': <tensorflow.python.keras.engine.sequential.Sequential at 0x20791aa0308>,\n",
       " 'A2': <tensorflow.python.keras.engine.sequential.Sequential at 0x207917dc3c8>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "918fcde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the models into an ensemble\n",
    "ensemble_predictions = []\n",
    "for model_label, model in models.items():\n",
    "    val_predictions = model.predict(encode_docs(tokenizer, max_len, val_data[\"Obs\"]))\n",
    "    ensemble_predictions.append(val_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c400386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of predictions for each input text\n",
    "ensemble_predictions = np.array(ensemble_predictions).squeeze().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4eefd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold ensemble predictions for binary classification\n",
    "binary_ensemble_predictions = (ensemble_predictions > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33d6336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          B3       0.27      0.94      0.42        33\n",
      "          B4       0.89      0.49      0.63        35\n",
      "          A3       0.67      0.13      0.22        31\n",
      "          B1       1.00      0.47      0.64        30\n",
      "          B2       0.96      0.81      0.88        27\n",
      "          A1       0.89      0.29      0.43        28\n",
      "          A2       0.71      0.65      0.68        26\n",
      "\n",
      "    accuracy                           0.54       210\n",
      "   macro avg       0.77      0.54      0.56       210\n",
      "weighted avg       0.76      0.54      0.55       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain precision, recall, F1-score, and print the classification report\n",
    "\n",
    "y_val = val_data[\"Type\"].apply(lambda x: list(models.keys()).index(x)).values\n",
    "y_hat = np.argmax((ensemble_predictions > 0.5).astype(int), axis=1)\n",
    "\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(y_val, y_hat, target_names=models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20ebf1",
   "metadata": {},
   "source": [
    "## Second Pass (Multi-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b649452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obs</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Observed child fixated on a particular texture...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patient's focus centers on vacuum cleaners, st...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Displays a strong interest in smelling various...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patient's attachment to a specific book is evi...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited awareness of personal boundaries, inva...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Displays a need for consistency in clothing ch...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Patient shows a strong attachment to a single ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Child displays a tendency to focus on irreleva...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>Displays a tendency to monologue about persona...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Engages in persistent rocking back and forth w...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Obs                   Type\n",
       "0     Observed child fixated on a particular texture...  [1, 0, 0, 0, 0, 0, 0]\n",
       "1     Patient's focus centers on vacuum cleaners, st...  [1, 0, 0, 0, 0, 0, 0]\n",
       "2     Displays a strong interest in smelling various...  [0, 1, 0, 0, 0, 0, 0]\n",
       "3     Patient's attachment to a specific book is evi...  [1, 0, 0, 0, 0, 0, 0]\n",
       "4     Limited awareness of personal boundaries, inva...  [0, 0, 1, 0, 0, 0, 0]\n",
       "...                                                 ...                    ...\n",
       "1045  Displays a need for consistency in clothing ch...  [0, 0, 0, 0, 1, 0, 0]\n",
       "1046  Patient shows a strong attachment to a single ...  [1, 0, 0, 0, 0, 0, 0]\n",
       "1047  Child displays a tendency to focus on irreleva...  [0, 0, 0, 0, 0, 0, 1]\n",
       "1048  Displays a tendency to monologue about persona...  [0, 0, 1, 0, 0, 0, 0]\n",
       "1049  Engages in persistent rocking back and forth w...  [0, 0, 0, 1, 0, 0, 0]\n",
       "\n",
       "[1050 rows x 2 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"Obs\"] = df[\"Obs\"]\n",
    "df2[\"Type\"] = [arr for arr in df[df[\"Type\"].unique()].values]\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "45e62515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "\n",
    "train_data, val_data = train_test_split(df2, test_size=0.2, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cda8f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your text data\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df2['Obs'])\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(len(sentence.split()) for sentence in df2['Obs'])\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e0f85d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12, 427,  87, ...,   0,   0,   0],\n",
       "       [ 14,  17,   5, ...,   0,   0,   0],\n",
       "       [ 12,   5,  32, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 14,  45,   2, ...,   0,   0,   0],\n",
       "       [523, 866,   1, ...,   0,   0,   0],\n",
       "       [ 11,   5,  48, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f85d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d0963f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B3': 0, 'B4': 1, 'A3': 2, 'B1': 3, 'B2': 4, 'A1': 5, 'A2': 6}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_to_index = dict(zip(df_['Type'].unique(), range(len(df_['Type'].unique()))))\n",
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eed72de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 7)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_labels = np.array([np.asarray(x, dtype = np.float64) for x in train_data[\"Type\"]])\n",
    "binary_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "14c4354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 26)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = encode_docs(tokenizer, max_len, train_data[\"Obs\"])\n",
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2d9ede67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-label deep learning model\n",
    "\n",
    "def create_multi_label_model(embedding_dim=50, num_classes=7):  # Set appropriate values for embedding_dim and num_classes\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')  # Use sigmoid activation for multi-label classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "21e266cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/15\n",
      "672/672 [==============================] - 1s 2ms/sample - loss: 0.5273 - accuracy: 0.8408 - val_loss: 0.4176 - val_accuracy: 0.8571\n",
      "Epoch 2/15\n",
      "672/672 [==============================] - 0s 257us/sample - loss: 0.4074 - accuracy: 0.8571 - val_loss: 0.4011 - val_accuracy: 0.8571\n",
      "Epoch 3/15\n",
      "672/672 [==============================] - 0s 253us/sample - loss: 0.3760 - accuracy: 0.8571 - val_loss: 0.3780 - val_accuracy: 0.8571\n",
      "Epoch 4/15\n",
      "672/672 [==============================] - 0s 257us/sample - loss: 0.3308 - accuracy: 0.8576 - val_loss: 0.3461 - val_accuracy: 0.8605\n",
      "Epoch 5/15\n",
      "672/672 [==============================] - 0s 265us/sample - loss: 0.2682 - accuracy: 0.8776 - val_loss: 0.2962 - val_accuracy: 0.8716\n",
      "Epoch 6/15\n",
      "672/672 [==============================] - 0s 269us/sample - loss: 0.1980 - accuracy: 0.9203 - val_loss: 0.2568 - val_accuracy: 0.8903\n",
      "Epoch 7/15\n",
      "672/672 [==============================] - 0s 310us/sample - loss: 0.1379 - accuracy: 0.9598 - val_loss: 0.2218 - val_accuracy: 0.9022\n",
      "Epoch 8/15\n",
      "672/672 [==============================] - 0s 350us/sample - loss: 0.0945 - accuracy: 0.9787 - val_loss: 0.2020 - val_accuracy: 0.9133\n",
      "Epoch 9/15\n",
      "672/672 [==============================] - 0s 312us/sample - loss: 0.0625 - accuracy: 0.9887 - val_loss: 0.1932 - val_accuracy: 0.9226\n",
      "Epoch 10/15\n",
      "672/672 [==============================] - 0s 321us/sample - loss: 0.0424 - accuracy: 0.9951 - val_loss: 0.1914 - val_accuracy: 0.9235\n",
      "Epoch 11/15\n",
      "672/672 [==============================] - 0s 319us/sample - loss: 0.0294 - accuracy: 0.9979 - val_loss: 0.1898 - val_accuracy: 0.9269\n",
      "Epoch 12/15\n",
      "672/672 [==============================] - 0s 303us/sample - loss: 0.0206 - accuracy: 0.9989 - val_loss: 0.1939 - val_accuracy: 0.9260\n",
      "Epoch 13/15\n",
      "672/672 [==============================] - 0s 305us/sample - loss: 0.0153 - accuracy: 0.9991 - val_loss: 0.1981 - val_accuracy: 0.9269\n",
      "Epoch 14/15\n",
      "672/672 [==============================] - 0s 402us/sample - loss: 0.0116 - accuracy: 0.9998 - val_loss: 0.2009 - val_accuracy: 0.9252\n",
      "Epoch 15/15\n",
      "672/672 [==============================] - 0s 331us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2079a155508>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a multi-label model\n",
    "num_classes = 7  # Number of unique classes (A1, A2, A3, B1, B2, B3, B4)\n",
    "\n",
    "multi_label_model = create_multi_label_model()\n",
    "multi_label_model.fit(encoded_data, binary_labels, epochs=15, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "# model.save('multi_label_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c30fb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# loaded_model = keras.models.load_model('multi_label_model.h5')\n",
    "\n",
    "# Predictions on validation set\n",
    "val_binary_labels = np.array([np.asarray(x, dtype = np.float64) for x in val_data[\"Type\"]])\n",
    "\n",
    "val_predictions = multi_label_model.predict(encode_docs(tokenizer, max_len, val_data['Obs']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4f974e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Threshold predictions for binary classification\n",
    "binary_val_predictions = (val_predictions > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4cac8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          B3       0.94      0.52      0.67        33\n",
      "          B4       0.91      0.57      0.70        35\n",
      "          A3       0.62      0.16      0.26        31\n",
      "          B1       0.95      0.67      0.78        30\n",
      "          B2       0.95      0.67      0.78        27\n",
      "          A1       0.79      0.39      0.52        28\n",
      "          A2       0.63      0.46      0.53        26\n",
      "\n",
      "   micro avg       0.85      0.49      0.62       210\n",
      "   macro avg       0.83      0.49      0.61       210\n",
      "weighted avg       0.83      0.49      0.61       210\n",
      " samples avg       0.49      0.49      0.49       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Obtain precision, recall, F1-score, and print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_binary_labels, binary_val_predictions, target_names=class_to_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05a658",
   "metadata": {},
   "source": [
    "## Third Pass (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136b7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.30.2\n",
      "Uninstalling transformers-4.30.2:\n",
      "  Successfully uninstalled transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch\n",
    "\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Assuming you have a DataFrame with 'text' column and 'labels' column\n",
    "# where 'labels' contains A1, A2, A3, B1, B2, B3, B4 labels as a comma-separated string\n",
    "# Replace 'your_dataset.csv' with your actual dataset file\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Convert comma-separated labels to lists\n",
    "df['labels'] = df['labels'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # You can try other BERT models as well\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(df['labels'].explode().unique()))\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "train_inputs = tokenizer(list(train_data['text']), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "val_inputs = tokenizer(list(val_data['text']), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "# Convert labels to tensor\n",
    "train_labels = torch.tensor([list(map(int, label)) for label in train_data['labels'].apply(lambda x: [1 if label in x else 0 for label in df['labels'].explode().unique()])])\n",
    "val_labels = torch.tensor([list(map(int, label)) for label in val_data['labels'].apply(lambda x: [1 if label in x else 0 for label in df['labels'].explode().unique()])])\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3  # You may need to adjust this based on your dataset size and convergence\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_model')\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_model')\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "# Evaluation on the validation set\n",
    "fine_tuned_model.eval()\n",
    "val_preds = []\n",
    "val_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        val_preds.append(preds.cpu().numpy())\n",
    "        val_true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and true labels\n",
    "val_preds = np.concatenate(val_preds, axis=0)\n",
    "val_true_labels = np.concatenate(val_true_labels, axis=0)\n",
    "\n",
    "# Obtain precision, recall, F1-score, and print the classification report\n",
    "print(\"Fine-tuned BERT Classification Report:\")\n",
    "print(classification_report(val_true_labels, val_preds, target_names=df['labels'].explode().unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22df225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154acfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea04f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
